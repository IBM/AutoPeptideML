{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"AutoPeptideML <p> AutoML system for building trustworthy peptide bioactivity predictors </p> <p> </p> <ul> <li>Documentation: https://ibm.github.io/AutoPeptideML</li> <li>Source Code: https://github.com/IBM/AutoPeptideML</li> <li>Webserver: http://peptide.ucd.ie/autopeptideml</li> <li>Google Collaboratory Notebook: AutoPeptideML_Collab.ipynb</li> <li>Blog post: Portal - AutoPeptideML v. 1.0 Tutorial</li> <li>Papers: </li> <li>AutoPeptideML (v. 1.0)</li> <li>ML Generalization from standard to modified peptides</li> </ul> <p>AutoPeptideML allows researchers without prior knowledge of machine learning to build models that are:</p> <ul> <li>Trustworthy: Robust evaluation following community guidelines for ML evaluation reporting in life sciences DOME.</li> <li>Interpretable: Output contains a PDF summary of the model evaluation explaining how to interpret the results to understand how reliable the model is.</li> <li>Reproducible: Output contains all necessary information for other researchers to reproduce the training and verify the results.</li> <li>State-of-the-art: Models generated with this system are competitive with state-of-the-art handcrafted approaches.</li> </ul> <p>To use version 1.0, which may be necessary for retrocompatibility with previously built models, please defer to the branch: AutoPeptideML v.1.0.6</p>"},{"location":"#contents","title":"Contents","text":"Table of Contents <ul> <li>Model builder</li> <li>Prediction</li> <li>Benchmark Data</li> <li>Intallation Guide</li> <li>Documentation</li> <li>License</li> <li>Acknowledgements </li> </ul>"},{"location":"#model-builder","title":"Model builder","text":"<p>In order to build a new model, AutoPeptideML (v.2.0), introduces a new utility to automatically prepare an experiment configuration file, to i) improve the reproducibility of the pipeline and ii) to keep a user-friendly interface despite the much increased flexibility.</p> <p><pre><code>autopeptideml prepare-config --config-path &lt;config-path&gt;\n</code></pre> This launches an interactive CLI that walks you through:</p> <ul> <li>Choosing a modeling task (classification or regression)</li> <li>Loading and parsing datasets (csv, tsv, or fasta)</li> <li>Picking models and representations</li> <li>Automatically sampling negatives</li> </ul> <p>You\u2019ll be prompted to answer various questions like:</p> <pre><code>- What is the modelling problem you're facing? (Classification or Regression)\n\n- How do you want to define your peptides? (Macromolecules or Sequences)\n\n- What models would you like to consider? (knn, adaboost, rf, etc.)\n</code></pre> <p>And so on. The final config is written to:</p> <pre><code>&lt;config-path&gt;.yml\n</code></pre> <p>This config file allows for easy reproducibility of the results, so that anyone can repeat the training processes. You can check the configuration file and make any changes you deem necessary. Finally, you can build the model by simply running:</p> <pre><code>autopeptideml build-model --outdir &lt;outdir&gt; --config-path &lt;outputdir&gt;/config.yml\n</code></pre>"},{"location":"#prediction","title":"Prediction","text":"<p>In order to use a model that has already built you can run:</p> <pre><code>autopeptideml predict &lt;result_dir&gt; &lt;features_path&gt; --feature-field &lt;feature_field&gt; --output-path &lt;my_predictions_path.csv&gt;\n</code></pre> <p>Where <code>&lt;features_path&gt;</code> is the path to a <code>CSV</code> file with a column <code>&lt;features_field&gt;</code> that contains the peptide sequences/SMILES. The output file <code>&lt;my_predictions_path&gt;</code> will contain the original data with two additional columns <code>score</code> (which are the predictions) and <code>std</code> which is the standard deviation between the predictions of the models in the ensemble, which can be used as a measure of the uncertainty of the prediction.</p>"},{"location":"#benchmark-data","title":"Benchmark data","text":"<p>Data used to benchmark our approach has been selected from the benchmarks collected by Du et al, 2023. A new set of benchmarks was constructed from the original set following the new data acquisition and dataset partitioning methods within AutoPeptideML. To download the datasets:</p> <ul> <li>Original UniDL4BioPep Benchmarks: Please check the project Github Repository.</li> <li>\u26a0\ufe0f New AutoPeptideML Benchmarks (Amended version): Can be downloaded from this link. Please note that these are not exactly the same benchmarks as used in the paper (see Issue #24 for more details).</li> <li>PeptideGeneralizationBenchmarks: Benchmarks evaluating how peptide representation methods generalize from canonical (peptides composed of the 20 standard amino acids) to non-canonical (peptides with non-standard amino acids or other chemical modifications). Check out the paper pre-print. They have their own dedicated repository: PeptideGeneralizationBenchmarks Github repository.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Installing in a conda environment is recommended. For creating the environment, please run:</p> <pre><code>conda create -n autopeptideml python\nconda activate autopeptideml\n</code></pre>"},{"location":"#1-python-package","title":"1. Python Package","text":""},{"location":"#11from-pypi","title":"1.1.From PyPI","text":"<pre><code>pip install autopeptideml\n</code></pre>"},{"location":"#12-directly-from-source","title":"1.2. Directly from source","text":"<pre><code>pip install git+https://github.com/IBM/AutoPeptideML\n</code></pre>"},{"location":"#2-third-party-dependencies","title":"2. Third-party dependencies","text":"<p>To use MMSeqs2 https://github.com/steineggerlab/mmseqs2</p> <pre><code># static build with AVX2 (fastest) (check using: cat /proc/cpuinfo | grep avx2)\nwget https://mmseqs.com/latest/mmseqs-linux-avx2.tar.gz; tar xvfz mmseqs-linux-avx2.tar.gz; export PATH=$(pwd)/mmseqs/bin/:$PATH\n\n# static build with SSE4.1  (check using: cat /proc/cpuinfo | grep sse4)\nwget https://mmseqs.com/latest/mmseqs-linux-sse41.tar.gz; tar xvfz mmseqs-linux-sse41.tar.gz; export PATH=$(pwd)/mmseqs/bin/:$PATH\n\n# static build with SSE2 (slowest, for very old systems)  (check using: cat /proc/cpuinfo | grep sse2)\nwget https://mmseqs.com/latest/mmseqs-linux-sse2.tar.gz; tar xvfz mmseqs-linux-sse2.tar.gz; export PATH=$(pwd)/mmseqs/bin/:$PATH\n\n# MacOS\nbrew install mmseqs2  \n</code></pre> <p>To use Needleman-Wunch, either:</p> <p><pre><code>conda install -c bioconda emboss\n</code></pre>   or</p> <pre><code>sudo apt install emboss\n</code></pre> <p>To use ECFP fingerprints:</p> <pre><code>pip install rdkit\n</code></pre> <p>To use MAPc fingeprints:</p> <pre><code>pip install mapchiral\n</code></pre> <p>To use PepFuNN fingeprints:</p> <pre><code>pip install git+https://github.com/novonordisk-research/pepfunn\n</code></pre> <p>To use PeptideCLM:</p> <pre><code>pip install smilesPE\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#configuration-file","title":"Configuration file","text":"<pre><code>datasets:\n  main:\n    feat-fields: # Column with peptide sequence/SMILES\n    label-field: # Column with labels/ \"Assume all entries are positives\"\n    path: # Path to dataset\n  neg-db:\n    activities-to-exclude: # List of activities to exclude\n      - activity-1\n      - activity-2\n      ...\n    feat-fields: null # Column with peptide sequence/SMILES (only if using custom database)\n    path: # Path to custom database or choose: canonical, non-canonical, both\ndevice: # Device for computing representations. Choose: cpu, mps, cuda\ndirection: # Direction of optimization. Choose: maximize or minimize\nmetric: # Metric for optimization. mse, mae require direction minimize\nmodels: # List of machine learning algorithms to explore. List:\n        # knn, svm, rf, gradboost, xgboost, lightgbm\n  - model-1\n  - model-2\n  ...\nn-trials: # Number of optimization steps. Recommended 100-200\npipeline: to-smiles # Pipeline for preprocessing. Choose: to-smiles, to-sequences\nreps: # List of peptide representations to explore. List:\n      # ecfp, chemberta-2, molformer-xl, peptide-clm, esm2-8m, ...\n  - rep-1\n  - rep-2\n  ...\n\nsplit-strategy: min # Strategy for splitting train/test. Choose: min, random. \ntask: class # Machine learning type of problem. Choose: class or reg.\nn-jobs: # Number of processes to launch. -1 uses all possible CPU cores.\n</code></pre>"},{"location":"#more-details-about-api","title":"More details about API","text":"<p>Please check the Code reference documentation</p>"},{"location":"#license","title":"License","text":"<p>AutoPeptideML is an open-source software licensed under the MIT Clause License. Check the details in the LICENSE file.</p>"},{"location":"#credits","title":"Credits","text":"<p>Special thanks to Silvia Gonz\u00e1lez L\u00f3pez for designing the AutoPeptideML logo and to Marcos Mart\u00ednez Galindo for his aid in setting up the AutoPeptideML webserver.</p>"},{"location":"autopeptideml/","title":"Class AutoPeptideML","text":""},{"location":"autopeptideml/#overview","title":"Overview","text":"<p><code>AutoPeptideML</code> is a configurable machine learning workflow class designed for peptide modeling. It integrates data pipelines, representations, model training (with HPO), evaluation, and export.</p>"},{"location":"autopeptideml/#class-autopeptideml_1","title":"Class: <code>AutoPeptideML</code>","text":""},{"location":"autopeptideml/#constructor","title":"Constructor","text":"<pre><code>AutoPeptideML(config: dict)\n</code></pre> <ul> <li>Initializes the AutoPeptideML workflow with a provided configuration dictionary.</li> <li>Creates output directories and stores pipeline, representation, training, and database settings.</li> </ul>"},{"location":"autopeptideml/#public-methods","title":"Public Methods","text":""},{"location":"autopeptideml/#get_pipeline","title":"<code>get_pipeline</code>","text":"<pre><code>get_pipeline(pipe_config: Optional[dict] = None) -&gt; Pipeline\n</code></pre> <p>Load or construct the preprocessing pipeline.</p>"},{"location":"autopeptideml/#get_database","title":"<code>get_database</code>","text":"<pre><code>get_database(db_config: Optional[dict] = None) -&gt; Database\n</code></pre> <p>Create or load the peptide database with optional negative data support.</p>"},{"location":"autopeptideml/#get_reps","title":"<code>get_reps</code>","text":"<pre><code>get_reps(rep_config: Optional[dict] = None) -&gt; Tuple[Dict[str, RepEngineBase], Dict[str, np.ndarray]]\n</code></pre> <p>Load or compute representations for the data.</p>"},{"location":"autopeptideml/#get_test","title":"<code>get_test</code>","text":"<pre><code>get_test(test_config: Optional[Dict] = None) -&gt; HestiaGenerator\n</code></pre> <p>Partition the dataset into training/validation/test using <code>HestiaGenerator</code>.</p>"},{"location":"autopeptideml/#get_train","title":"<code>get_train</code>","text":"<pre><code>get_train(train_config: Optional[Dict] = None) -&gt; BaseTrainer\n</code></pre> <p>Load and return the trainer based on the configuration (supports Optuna and Grid).</p>"},{"location":"autopeptideml/#run_hpo","title":"<code>run_hpo</code>","text":"<pre><code>run_hpo() -&gt; Dict\n</code></pre> <p>Perform hyperparameter optimization across dataset partitions.</p>"},{"location":"autopeptideml/#run_evaluation","title":"<code>run_evaluation</code>","text":"<pre><code>run_evaluation(models) -&gt; pd.DataFrame\n</code></pre> <p>Run evaluation on the trained models and return a DataFrame of results.</p>"},{"location":"autopeptideml/#save_experiment","title":"<code>save_experiment</code>","text":"<pre><code>save_experiment(model_backend: str = 'onnx', save_reps: bool = False, save_test: bool = True, save_all_models: bool = True)\n</code></pre> <p>Save the full experiment including models, test partitions, and configuration.</p>"},{"location":"autopeptideml/#save_database","title":"<code>save_database</code>","text":"<pre><code>save_database()\n</code></pre> <p>Export the database to CSV.</p>"},{"location":"autopeptideml/#save_models","title":"<code>save_models</code>","text":"<pre><code>save_models(ensemble_path: str, backend: str = 'onnx', save_all: bool = True)\n</code></pre> <p>Save models using <code>onnx</code> or <code>joblib</code> backends.</p>"},{"location":"autopeptideml/#save_reps","title":"<code>save_reps</code>","text":"<pre><code>save_reps(rep_dir: str)\n</code></pre> <p>Save precomputed representations to disk.</p>"},{"location":"autopeptideml/#predict","title":"<code>predict</code>","text":"<pre><code>predict(df: pd.DataFrame, feature_field: str, experiment_dir: str, backend: str = 'onnx') -&gt; np.ndarray\n</code></pre> <p>Load a saved experiment and predict using the trained ensemble on new data.</p>"},{"location":"autopeptideml/#configuration-keys","title":"Configuration Keys","text":"<p>The <code>config</code> dictionary passed to the constructor must include the following keys:</p> <ul> <li><code>outputdir</code>: str</li> <li><code>pipeline</code>: dict or str</li> <li><code>representation</code>: dict or str</li> <li><code>train</code>: dict or str</li> <li><code>databases</code>: dict</li> <li><code>test</code>: dict</li> </ul>"},{"location":"autopeptideml/#dependencies","title":"Dependencies","text":"<ul> <li>pandas, numpy</li> <li>yaml, json</li> <li>hestia</li> <li>sklearn</li> <li>skl2onnx, onnxmltools, joblib (optional)</li> </ul>"},{"location":"autopeptideml/#example-usage","title":"Example Usage","text":"<pre><code>from autopipeline.autopeptideml import AutoPeptideML\n\nconfig = yaml.safe_load(open('config.yml'))\nrunner = AutoPeptideML(config)\npipeline = runner.get_pipeline()\ndb = runner.get_database()\nreps, x = runner.get_reps()\ntest = runner.get_test()\ntrainer = runner.get_train()\nmodels = runner.run_hpo()\nevaluation = runner.run_evaluation(models)\nrunner.save_experiment()\n</code></pre> <p>For detailed config templates and supported options, see the corresponding YAML schema documentation.</p>"},{"location":"repenginebase/","title":"<code>RepEngineBase</code> Class Documentation","text":"<p>Module: <code>rep_engine_base</code></p>"},{"location":"repenginebase/#purpose","title":"Purpose","text":"<p><code>RepEngineBase</code> is an abstract base class for molecular representation engines. It defines a standard interface and utilities for computing molecular representations from a list of molecules (e.g., SMILES strings), particularly in batched processing. This class is intended to be subclassed, with core functionality like preprocessing and representation computation implemented in derived classes.</p>"},{"location":"repenginebase/#attributes","title":"Attributes","text":"<ul> <li> <p><code>engine</code> (<code>str</code>):   Name of the representation engine. Typically defined in a subclass or passed during instantiation.</p> </li> <li> <p><code>rep</code> (<code>str</code>):   Type of molecular representation (e.g., <code>'fingerprint'</code>, <code>'embedding'</code>).</p> </li> <li> <p><code>properties</code> (<code>dict</code>):   A deep copy of the instance's dictionary at initialization. Captures configuration state.</p> </li> </ul>"},{"location":"repenginebase/#constructor","title":"Constructor","text":"<pre><code>def __init__(self, rep: str, **args)\n</code></pre> <p>Parameters: - <code>rep</code> (<code>str</code>): Type of molecular representation. - <code>**args</code> (<code>dict</code>): Additional configuration options stored as attributes.</p> <p>Effect: Initializes the object, stores <code>rep</code>, and adds all additional keyword arguments to the instance. Also creates a deep copy of all these attributes in <code>self.properties</code> for serialization.</p>"},{"location":"repenginebase/#public-methods","title":"Public Methods","text":""},{"location":"repenginebase/#compute_reps","title":"<code>compute_reps</code>","text":"<pre><code>def compute_reps(self, mols: List[str], verbose: Optional[bool] = False, batch_size: Optional[int] = 12) -&gt; Union[np.ndarray, List[np.ndarray]]\n</code></pre> <p>Description: Computes molecular representations in batches using <code>_preprocess_batch</code> and <code>_rep_batch</code>.</p> <p>Parameters: - <code>mols</code> (<code>List[str]</code>): List of molecular inputs (e.g., SMILES strings). - <code>verbose</code> (<code>bool</code>, optional): If <code>True</code>, shows a progress bar. - <code>batch_size</code> (<code>int</code>, optional): Number of molecules per batch.</p> <p>Returns: - <code>np.ndarray</code> if <code>average_pooling</code> is <code>True</code> or unset. - <code>List[np.ndarray]</code> if <code>average_pooling</code> is explicitly set to <code>False</code>.</p>"},{"location":"repenginebase/#dim","title":"<code>dim</code>","text":"<pre><code>def dim(self) -&gt; int\n</code></pre> <p>Description: Abstract method. Must return the dimensionality of the computed representation.</p> <p>Raises: - <code>NotImplementedError</code></p>"},{"location":"repenginebase/#_rep_batch","title":"<code>_rep_batch</code>","text":"<pre><code>def _rep_batch(self, batch: List[str]) -&gt; np.ndarray\n</code></pre> <p>Description: Abstract method. Must compute and return the representation for a batch of molecules.</p> <p>Raises: - <code>NotImplementedError</code></p>"},{"location":"repenginebase/#_preprocess_batch","title":"<code>_preprocess_batch</code>","text":"<pre><code>def _preprocess_batch(self, batch: List[str]) -&gt; List[str]\n</code></pre> <p>Description: Abstract method. Must return a preprocessed version of the batch for representation.</p> <p>Raises: - <code>NotImplementedError</code></p>"},{"location":"repenginebase/#save","title":"<code>save</code>","text":"<pre><code>def save(self, filename: str)\n</code></pre> <p>Description: Serializes and saves the engine\u2019s properties to a YAML file.</p> <p>Parameters: - <code>filename</code> (<code>str</code>): Destination path for the YAML file.</p>"},{"location":"repenginebase/#design-notes","title":"Design Notes","text":"<ul> <li>This class provides batch processing support and optional average pooling control.</li> <li>The use of <code>batched</code> from <code>itertools</code> supports Python 3.10+ but also includes a fallback implementation for older versions.</li> <li>Intended for extension: Subclasses must implement <code>_rep_batch</code>, <code>_preprocess_batch</code>, and <code>dim</code>.</li> </ul>"},{"location":"repenginefp/","title":"RepEngineFP","text":"<p>               Bases: <code>RepEngineBase</code></p> <p>Class <code>RepEngineFP</code> is a subclass of <code>RepEngineBase</code> designed for computing molecular fingerprints (FPs)  using popular fingerprinting algorithms such as ECFP or FCFP. This engine generates fixed-length bit vectors  representing molecular structures based on their topological features.</p> <p>Attributes:     :type engine: str     :param engine: The name of the engine. Default is <code>'fp'</code>, indicating a fingerprint-based representation.</p> <pre><code>:type nbits: int\n:param nbits: The length of the fingerprint bit vector. This determines the number of bits in the fingerprint.\n\n:type radius: int\n:param radius: The radius parameter used for fingerprint generation, determining the neighborhood size around each atom.\n\n:type name: str\n:param name: The name of the fingerprint generator, which includes the engine type, `nbits`, and `radius`.\n\n:type generator: object\n:param generator: The fingerprint generator object, loaded based on the specified `rep` type.\n</code></pre> <p>Initializes the <code>RepEngineFP</code> with the specified representation type, fingerprint size, and radius.</p>"},{"location":"repenginefp/#autopeptideml.reps.fps.RepEngineFP.dim","title":"<code>dim()</code>","text":"<p>Returns the dimensionality (bit size) of the generated fingerprint.</p>"},{"location":"repenginelm/","title":"RepEngineLM","text":"<p>               Bases: <code>RepEngineBase</code></p> <p>Class <code>RepEngineLM</code> is a subclass of <code>RepEngineBase</code> designed to compute molecular representations  using pre-trained language models (LMs) such as T5, ESM, or ChemBERTa. This engine generates vector-based  embeddings for input sequences, typically protein or peptide sequences, by leveraging transformer-based models.</p> <p>Attributes:     :type engine: str     :param engine: The name of the engine. Default is <code>'lm'</code>, indicating a language model-based representation.</p> <pre><code>:type device: str\n:param device: The device on which the model runs, either `'cuda'` for GPU or `'cpu'`.\n\n:type model: object\n:param model: The pre-trained model used for generating representations. The model is loaded from a repository \n                based on the `model` parameter.\n\n:type name: str\n:param name: The name of the model engine combined with the model type.\n\n:type dimension: int\n:param dimension: The dimensionality of the output representation, corresponding to the model's embedding size.\n\n:type model_name: str\n:param model_name: The specific model name used for generating representations.\n\n:type tokenizer: object\n:param tokenizer: The tokenizer associated with the model, used for converting sequences into tokenized input.\n\n:type lab: str\n:param lab: The laboratory or organization associated with the model (e.g., 'Rostlab', 'facebook', etc.).\n</code></pre> <p>Initializes the <code>RepEngineLM</code> with the specified model and pooling options. The model is loaded based on  the given <code>model</code> name and its associated tokenizer.</p>"},{"location":"repenginelm/#autopeptideml.reps.lms.RepEngineLM.dim","title":"<code>dim()</code>","text":"<p>Returns the dimensionality of the output representation generated by the model.</p>"},{"location":"repenginelm/#autopeptideml.reps.lms.RepEngineLM.get_num_params","title":"<code>get_num_params()</code>","text":"<p>Returns the total number of parameters in the model.</p>"},{"location":"repenginelm/#autopeptideml.reps.lms.RepEngineLM.max_len","title":"<code>max_len()</code>","text":"<p>Returns the maximum allowed sequence length for the model. Some models have a specific maximum sequence length.</p>"},{"location":"repenginelm/#autopeptideml.reps.lms.RepEngineLM.move_to_device","title":"<code>move_to_device(device)</code>","text":"<p>Moves the model to the specified device (e.g., 'cuda' or 'cpu').</p>"},{"location":"repengineseqbased/","title":"RepEngineOneHotEncoding","text":"<p>               Bases: <code>RepEngineBase</code></p> <p>Class <code>RepEngineOnehot</code> is a subclass of <code>RepEngineBase</code> that generates one-hot encoded representations  for input sequences. This representation is commonly used for tasks in machine learning and bioinformatics,  such as protein sequence classification, where each amino acid in the sequence is represented by a binary vector.</p> <p>Attributes:     :type engine: str     :param engine: The name of the engine. Default is <code>'one-hot'</code>, indicating one-hot encoding representation.</p> <pre><code>:type max_length: int\n:param max_length: The maximum length of the input sequences. Sequences longer than this length will be truncated.\n\n:type name: str\n:param name: The name of the representation engine, which is set to `'one-hot'`.\n</code></pre> <p>Initializes the <code>RepEngineOnehot</code> with the specified maximum sequence length. The one-hot encoding will use this length to determine the size of the output vectors.</p>"},{"location":"repengineseqbased/#autopeptideml.reps.seq_based.RepEngineOnehot.dim","title":"<code>dim()</code>","text":"<p>Returns the dimensionality of the one-hot encoded representation, which is the product of the  maximum sequence length and the number of possible amino acids.</p>"}]}